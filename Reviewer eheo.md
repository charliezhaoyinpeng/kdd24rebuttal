**[Q1]** The goal of the proposed method in this paper is to learn a fairness-aware invariant classifier $f$ based on given source data that can generalize well on target domains during inference. To this end, enhancement of the diversity of source data through the transformation model $T$ becomes the key of our method. Throughout this paper, we claim that variations across domains due to covariate and dependence shifts simultaneously, where covariate and dependence shifts stem from different sample styles (e.g., photos and arts) and correlations ($\rho$) between class labels and sensitive attributes, respectively. 

Augmented images (via $T$) with random style and sensitive factors enhance the diversity of source data in synthetic domains, where a domain is represented by a unique style factor and a dependence score (i.e., correlation $\rho$), denoted in lines 394-397. Furthermore, such generated images are used as regularizers when learning $f$, to ensure class invariance ($L_{inv}$ in step 7 of Algorithm 1) and mitigate model unfairness ($L_{fair}$ in step 9 of Algorithm 1).

**[Q2]** This is a great question. In our method, we do not use semantic factors as input to learn a fair classifier, but instead, the classifier $f$ is learned using original data followed by invariant and fair regularizers (line 10 in Algorithm 1). This is because the semantic encoder is learned based on a limited number of source domains (e.g., 3 source domains in the PACS dataset). However, in our method, many data from synthetic domains are generated via $T$. Such data and original ones (from >>3 domains) are used together ($L_{inv}$ and $L_{fair}$) to regularize the classifier to enhance the invariance of $f$. The greater the diversity within source data, the stronger the potential for generalization of $f$.

**[Q3]** The similarity metrics could be different. By following [20] and [57], we choose l1-norm loss in the reconstruction loss.

**[Q4]** Different from the sensitive factor $a$ and the style factor $s$ encoded from the images, $a’$ and $s’$ are sampled from their prior Gaussian distribution to enhance the diversity of source domains. As indicated in lines 594-596 of section 4.3 and step 17 of Algorithm 1, $a’$ and $s’$ are randomly sampled rather than encoded through their encoders.

**[Q5]** Thanks for the comment. The generated images of the Fairface dataset, shown in Fig. 3, are decoded using the unchanged semantic factor $c$ and randomly sampled style and sensitive factors $s’$ and $a’$.  Therefore, sensitive attributes $z=h(a’)$ of these images are not guaranteed to remain the same as the original images or to alter to a different one. Notice that augmenting images in synthetic domains aims to enhance the diversity of source data with respect to diverse styles and dependence scores rather than converting images with opposite sensitive attributes. Besides, to indicate the effectiveness of our augmentation strategy, we use ColorJitter as one of our baselines. In this baseline, images are augmented by random brightness, contrast, saturation, and hue. In contrast to our method, this method neither ensures the invariance of semantics nor makes targeted changes in style and sensitive factors. Our results in Tables 4 and 5 show that our method outperforms the ColorJitter in both fairness and accuracy. 

**[Q6]** The bidirectional reconstruction loss is the key to ensuring the disentanglement of three factors. This loss is extended from a highly cited image-to-image translation work MUNIT [20] (2736 citations), that has been commercialized by NVIDIA. In the factor reconstruction loss, specifically line 445, with a style factor $s$ sampled from its prior Gaussian and the semantic as well as the sensitive factors encoded from the sample $\mathbf{x}$, $s$ is required to be reconstructed (line 445). This is similar to the reconstruction of a sensitive factor $a$ (line 444). 

Besides, to demonstrate the effectiveness of factor disentanglement, we conduct a parallel experiment (lines 806-848) and show the visualization in Fig. 4. In this experiment, we use ccMNIST as an example and train three transformation models for three domains separately, one model for each domain. As shown in Fig. 4, output images are generated using semantic (digit class), sensitive (background color), and style (digit color) factors from different domains. These results demonstrate the success of disentanglements from images by using different encoders.

Moreover, due to space limits, we defer the details of the disentanglement approach in Appendix B.2. Simply saying, this approach consists of two levels (Fig. 7). In the outer level, a sample is disentangled into a style factor $s$ and a content factor $m$ (with a focus on covariate shift). In the inner level, the content factor $m$ is further disentangled into a semantic factor $c$ and a sensitive factor $a$ (with a focus on dependence shift). Each level contains two encoders ($E_m, E_s$, and $E_c, E_a$), a decoder ($G_i$ and $G_o$), and a discriminator ($D_i$ and $D_o$). To learn these architectures, bidirectional reconstruction losses (lines 1250-1263) and the adversarial loss (lines 1308-1313) are iteratively optimized. This two-level approach ensures the success of three-factor disentanglement. As we mentioned above, since this approach is extended from MUNIT [20], we simply the learning process in the main paper.

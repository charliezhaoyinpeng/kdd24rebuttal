

**[Q1]** We need to clarify that our technical contributions extend beyond merely adding fair constraints. To learn a fair classifier under covariate and dependence shifts, **our technical contributions are threefold**:
- [sec. 4.1] A transformation model is proposed to disentangle inputs into semantic $c$, sensitive $a$, and style $s$ factors.
- [sec. 4.2] To enhance the diversity of source domains while ensuring fairness, we augment data by preserving $c$ unchanged and randomly sampling $\bf a$ and $\bf c$ through the transformation model $T$. Therefore, the generated data in each synthetic domain exhibit various styles (covariate shift) and dependence scores (dependence shift). 
- [sec. 4.2] A fairness-aware invariant classifier $f$ is learned based on the original source and augmented data. The objective function is regularized with $L_{inv}$ and $L_{fair}$, as stated in steps 7 and 9 in Algorithm 1. 

**[Q2]** Since $f$ is initially defined in a non-parametric space and optimized over all domains, we first approximate Problem 1 to Problem 2 (lines 504-509) and further reformulate Problem 2 to its dual problem Eq.(9) by relaxing the constraints of Problem 2 in a parametric space. The reformulation aims to address the challenges proposed in lines 510-519. Therefore, Theorem 1 aims to give the gap between theoretic and empirical solutions (lines 620-623). As we do not assume target data are known and accessible during train, given source domains, Theorem 2 aims to provide a fairness upper bound of target domains. Lines 634-637 and 667-675 provide conclusions for both theorems and proofs are given in Appendix D.

**[Q3]** The description of Tables 4 and 5 are given in lines 849-865. Presentations of these two tables are **frequently** used in domain generalization papers, such as [57,40,37]. Similarly, results shown in each column represent performance on the target domain, using the rest as source domains (lines 852-854). The “Avg” column indicates the average performance over all target domains. Interpretations of evaluation metrics are given in lines 747-776.

As shown in Table 4 (FairFace dataset), compared with baselines, our method has the best accuracy and fairness over all domains. Specifically, our method exhibits an average improvement in fairness (3% in DP, 2% in AUC_fair) while maintaining comparable accuracy (0.19% better). Similarly, in Table 5 (YFCC-100M-FDG dataset), our method demonstrates superior fairness (8% in DP, 4% in AUC_fair) and comparable accuracy (0.35% better) compared to the best baseline. These quantitive analyses are discussed in lines 860-865.

**[Q4]** DDG [57] and MBDG [40] are recent domain generalization approaches that have demonstrated superior performance compared to IRM and GDRO in generalization accuracy. Therefore, in our experiments, we include naive baselines, DDG+FC and MBDG+FC, in Tables 4,5, 7-10.  Notice that we use 19 baselines in our experiments (lines 730-747), and 7 of them are fairness-aware domain generalization methods. Our results based on 4 datasets demonstrate the effectiveness of our model.

**[Q5]** In recent years, the issue of controlling model fairness under distribution shifts has become a significant focus in the machine learning and data mining community. Given its increasing importance, we offer a brief survey of relevant literature outlined in Table 2 and present a specific problem through Fig. 1 in our paper.  To our knowledge, our paper marks the pioneering effort in mitigating unfairness under covariate and dependence shifts simultaneously.

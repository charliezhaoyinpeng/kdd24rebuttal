**[W1]** In Problem 1, $e$ is an element in the set of domain labels for all domains $\mathcal{E}$ and $s$ is an element in the set of domain labels for all source domains $\mathcal{E}^s$, where $\mathcal{E}^s$ is a subset of $\mathcal{E}$. We will refine this clarification in the accepted version. The goal of this problem is to learn a fair classifier based on the given source domains, and the classifier can be generalized well on all domains, including the source domains and unknown target domains.

**[W2]** In this paper, we use binary classification and binary sensitive attributes as an example to illustrate our idea. However, the framework can be generalized to multi-class, multi-sensitive attributes (lines 303-306). For example, we can replace Eq.(1) accordingly with a fair notion that takes multi-class and/or multi-sensitive attributes into consideration.

**[W3]** The disentanglement relies on encoders and decoders proposed in section 4.1. To learn such encoders and decoders, in lines 428-448, we gave a bidirectional loss decomposed of $L_{recon}^{data}$ and $L_{recon}^{factor}$, which are extended from a highly cited work MUNIT [20] (2736 citations) that has been commercialized by NVIDIA. This loss is the key component to disentangle a sample into three latent factors, where the loss is also a part of Eq.(3).

In $L_{recon}^{factor}$, with a style factor $s$ sampled from its prior Gaussian and the semantic as well as the sensitive factors encoded from the sample $\mathbf{x}$, $s$ is required to be reconstructed (line 444). This is similar to the reconstruction of sensitive factors $a$ (line 445).

Due to space limits, we defer the details of the disentanglement approach in Appendix B.2. Simply saying, this approach consists of two levels (Fig. 7). In the outer level, a sample is disentangled into a style factor $s$ and a content factor $m$ (with a focus on covariate shift). In the inner level, the content factor $m$ is further disentangled into a semantic factor $c$ and a sensitive factor $a$ (with a focus on dependence shift). Each level contains two encoders ($E_m, E_s$, and $E_c, E_a$), a decoder ($G_i$ and $G_o$), and a discriminator ($D_i$ and $D_o$). To learn these architectures, bidirectional reconstruction losses (lines 1250-1263) and the adversarial loss (lines 1308-1313) are iteratively optimized. This two-level approach ensures the success of three-factor disentanglement. As we mentioned above, since this approach is extended from MUNIT [20], we simply the learning process in the main paper.

**[W4]** We gave notations throughout this paper at the beginning of section 3 in lines 263-276.  Similar to $\mathbb{P}^e_{XYZ}$, $\mathbb{P}^{s_i}_{C|XZY}=\mathbb{P}(C^{s_i}|X^{s_i},Z^{s_i},Y^{s_i})$ represents the conditional distribution of the latent semantic variable $C$ given samples $(X,Z,Y)$ in the source domain $s_i$.

**[Q1]** Due to space limits, we defer the disentanglement algorithm (Algorithm 2, Learning the Transformation Model $T$) with details in Appendix B. We refer to this algorithm at the beginning of section 4.3 (lines 584-585). In the main algorithm (Algorithm 1), we assume $T$ is learned and ready to be used for augmentation (steps 15-20).

The idea of our augmentation strategy is to generate images to enhance the diversity of source data by remaining semantic factors associated with randomly sampled semantic and style factors. In contrast to the ccMNIST results in Fig.3, although FairFace results do not visually show the change in race (style) and gender (sensitive attribute) due to the complexity of images, the required enhancement of diversity with respect to the covariate and dependence shifts can be achieved. Therefore, we use ColorJitter as one of our baselines, in which images are augmented by random brightness, contrast, saturation, and hue. Our results in Tables 4 and 5 show that our method outperforms the ColorJitter in both fairness and accuracy.


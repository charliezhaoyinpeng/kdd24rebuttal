**[W1]** One real-world application of this work can be found in automated hiring systems. In such systems, a learned model is capable of making hiring decisions for employment candidates on behalf of a company, irrespective of sensitive information such as race. For instance, an automated hiring system can be trained using historical data collected from various geographical regions. Each data sample represents a candidate with multiple features, including age, race, education levels, past working experience, and so on. However, distributions of these data features may differ across regions (covariate shift), and the correlations between predicted outcomes (whether to extend a job offer or not) and sensitive attributes (such as race) may also vary across regions (dependence shift). In this scenario, our proposed approach aims to develop a fairness-aware classifier capable of generalizing from data sampled from given geographical regions to unknown ones, while simultaneously addressing potential covariate and dependence shifts.

**[W2]** Thanks for the suggestion. We will refine this clarification in the accepted version. In Table 2, $S$ is the same as $|\mathcal{E}^s|$ representing the number of source domains. Its values, 1 and $M$, indicate single and multiple domains. $\mathcal{D}_{tgt}$ represents target datasets.

**[W3]** The goal of Fig. 2 is to illustrate the learning of the transformation model (left) and the use of it for augmentation (right). Keyword summaries refer to the grey boxes at the bottom of Fig. 2 (above the caption). Since the generated samples in synthetic domains are further used to learn the classifier $f$, $f$ is not included in this figure.

**[W4]** We claim that the data augmentation strategy is part of the technical contribution of the proposed method, FEDORA. It serves as a key to learning the fair classifier $f$ in section 4.2. Technically, the main contributions of the proposed method are three folds: (1) we learn the transformation model and use it to enhance the diversity of source data (section 4.1); and, more importantly, (2) learn a classifier $f$ satisfying domain-invariance with respect to model fairness, and (3) approximate $f$ (due to challenges stated in lines 512-519) to an empirical solution with theoretic guarantees.

**[W5]** In this work, we address the fairness-aware domain generalization problem, specifically attributed to the distribution shifts due to covariate and dependence shifts simultaneously. To our knowledge, this problem is novel, as we show existing related works in Table 2. The main idea of the proposed method is to learn a fairness-aware invariant classifier by enhancing the diversity of data in source domains with respect to data styles and dependence scores between sensitive attributes and labels.

**[Q1]**  Taking the left-most figure of Fig. 1 as an example, squares (triangles) represent positive (negative) classes. Demographic parity, which is frequently used as a metric for group fairness, requires an equal proportion of positive predictions in each sensitive group (red border: $z=1$, black border: $z=-1$).  Therefore, the probability of the red group $\mathbb{P}(Y=1|Z=1)=2/4=1/2$ is equal to the one of the black group $\mathbb{P}(Y=1|Z=-1)=4/8=1/2$, which indicates $f$ is fair. Misclassification is inevitable when learning a fair classifier due to the trade-off between model accuracy and group fairness.

**[Q2]** Both the explanation in lines 126 to 133 and the “such hybrid shifts” in line 133 refer to the shift arising from covariate shift and dependence shift simultaneously. As shown in Fig. 1, the shift from source to target domains is due to changes in (1) image styles (covariate shift) and (2) correlations between labels and sensitive attributes (dependence shift).

**[Q3]** In Eq.(1), we provide a general form of group fairness. As indicated in lines 300-302, when $p_1$ is set to $\mathbb{P}(Z=1,Y=1)$, Eq.(1) becomes a relaxed form of difference of equalized opportunity (DEO), where DEO is defined as  $|\mathbb{P}(\hat{Y}=1|Z=1,Y=1) - \mathbb{P}(\hat{Y}=1|Z=-1,Y=1)|$ in [35].  We agree with your statement that DEO can be $0$ with all predictions being $0$ class, indicating perfect fairness but poor accuracy. Due to the tradeoff between fairness and accuracy in group fairness, one needs to improve predicted model fairness and ensure comparable accuracy performance, as our results are shown in Tables 4 and 5.

Besides, $Z$ in Eq.(1) requires to be {$-1,1$}. This is because $(\frac{Z+1}{2}-p_1)$ needs to be offset with the denominator in $\frac{1}{p_1(1-p_1)}$. However, it is OK for class labels lying in {$0,1$}.

**[Q4]** Due to the inaccessibility of target domains during training, Problem 1 aims to learn a fair classifier that can be generalized well on all domains (source and target domains) with given source domains only. The problem in practice is challenging because we need to find a $f$ based on source data and also make it reach a minimum risk onto unknown target data (lines 517-519, 561-564, 575-576).

**[Q5]** The similarity metrics could be different. By following [20] and [57], we choose l1-norm loss in the reconstruction loss. Following [40], we use KL-divergence in Eq.(7). 

**[Q6]** The bidirectional reconstruction loss (data and factor reconstructions) is the key to ensuring the disentanglement of latent factors. This loss is extended from a highly cited work MUNIT [20] (2736 citations), that has been commercialized by NVIDIA. As we defined in Assumption 1 (lines 394-397), each domain is represented by a unique style factor $s$ and a dependence score that is related to sensitive factors $a$. 

The change of styles and the dependence scores across domains results from covariate shifts and dependence shifts that characterize the extent to which $\mathbb{P}_X^e$ and $\rho(Y^e, Z^e)$ differ between domains.

In the factor reconstruction loss (lines 442-446), with a style factor $s$ sampled from its prior Gaussian and the semantic as well as the sensitive factors encoded from the sample $\mathbf{x}$, $s$ is required to be reconstructed (line 444). This is similar to the reconstruction of sensitive factors $a$ (line 445).


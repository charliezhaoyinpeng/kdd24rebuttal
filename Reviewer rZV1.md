**[Q1]** In lines 401-427 of section 4.1, we illustrate the functionality of the proposed transformation model $T$. For example, given a sample $(\mathbf{x}^{e_i},\mathbf{z}^{e_i},y)$ in domain $e_i$, the transformation model can generate its corresponding pair $(\mathbf{x}^{e_j},\mathbf{z}^{e_j},y)$ in a different domain $e_j$, maintaining the label $y$ while alternating the style and sensitive attribute, where $e_i \neq e_j$. This process is represented as $(\mathbf{x}^{e_j},\mathbf{z}^{e_j})=T(\mathbf{x}^{e_i},\mathbf{z}^{e_i}, e_j)$. 

An example is given on the right of Fig. 2. where a digit 7 (class label) with a blue digit color (style) and a black background (sensitive attribute) is generated through $T$ from its original digit with a green digit color (style) and white background (sensitive attribute). As a domain comprises many samples, changes in sensitive attributes lead to alternation in dependence (group fairness) between sensitive attributes and class labels.

This illustrates that a sample within domain $e_i$ can be converted to its corresponding pair within another domain $e_j$ through $T$, with a domain characterized by its style factor and the dependence score between sensitive attributes and class labels (lines 394-397). 

Additionally, in lines 273-276, we clarify that superscripts denote domain labels (e.g., $\mathcal{E}^s$ represents the set of source domain labels), while subscripts denote indices of encoders (e.g., $E_s$ represents style encoder). We apologize that there is a typo in line 320, where $\mathcal{E}_s$ should be corrected to $\mathcal{E}^s$. This correction will be made in the accepted version.

**[Q2]** As we do not assume that target domains are known or accessible during training, the goal of Eq.(2) is to train a fairness-aware classifier $f$ using the given training domains. This classifier should demonstrate robust generalization across all domains, encompassing both known source domains and any unknown target domains.

**[Q3]** The bidirectional reconstruction loss (data and factor reconstructions) is a key mechanism for ensuring the disentanglement of three factors. This loss is an extension derived from the influential work MUNIT [20] (2736 citations) that has been commercialized by NVIDIA. As we defined in Assumption 1 (lines 394-397), each domain is characterized by a unique style factor $s$ and a dependence score associated with sensitive factors $a$. 

In the factor reconstruction loss (lines 442-446), a style factor $s$ sampled from its prior Gaussian distribution along with the semantic and sensitive factors encoded from the sample $\mathbf{x}$, $s$ is reconstructed (line 444). Similarly, the reconstruction of the sensitive factor $a$ is given in line 445. Furthermore, the sensitiveness loss contributes to the disentanglement of $a$. 

Moreover, the disentanglement of $c$ from $a$ is necessary. This stems from the definition of fairness, which mandates that class labels be solely dependent on sample semantics and remain independent of sensitive information. Although a factor combining $a$ and $c$ may result in a decrease in sensitiveness loss and reconstruction of the data, failing to segregate the semantic and sensitive encoders could compromise model fairness.

Due to space limits, we defer the details of the disentanglement approach in Appendix B.2. Simply saying, this approach consists of two levels (Fig. 7). In the outer level, a sample is disentangled into a style factor $s$ and a content factor $m$ (with a focus on covariate shift). In the inner level, the content factor $m$ is further disentangled into a semantic factor $c$ and a sensitive factor $a$ (with a focus on dependence shift). Each level contains two encoders ($E_m, E_s$, and $E_c, E_a$), a decoder ($G_i$ and $G_o$), and a discriminator ($D_i$ and $D_o$). To learn these architectures, bidirectional reconstruction losses (lines 1250-1263) and the adversarial loss (lines 1308-1313) are iteratively optimized. This two-level approach ensures the success of three-factor disentanglement. As we mentioned above, since this approach is extended from MUNIT [20], we simply the learning process in the main paper.



**[Q4]** In our method, we do not use semantic factors as input to learn a fair classifier. Instead, the classifier $f$ is learned using original data followed by invariant and fair regularizers (step 10 of Algorithm 1). This is because the semantic encoder is trained based on a limited number of source domains (e.g., 3 source domains in the PACS dataset). However, in our method, data from numerous synthetic domains are generated via $T$. Such data and original ones (from >>3 domains) are used together ($L_{inv}$ and $L_{fair}$) to regularize the classifier to enhance the invariance of $f$. The greater the diversity within source data, the stronger the potential for generalization of $f$. 

**[Q5]** Our method achieves fairness through 
- (1) Enhancing the diversity of dependence scores $\rho$ in synthetic domains generated via $T$. As shown in Fig. 2, an augmented sample featuring a random sensitive factor is endowed with a sensitive attribute by $h$. Therefore, while retaining unchanged class labels, the dependence score in a synthetic domain is changed. 
- (2) A fair regularizer $L_{fair}$ defined in accordance with Definition 3 facilitates the learning of group fairness across source and synthetic domains. 

Moreover, DP as a fair metric requires an equal proportion (ratio) of positive predictions in each sensitive group. Therefore, the higher ratio represents better fair performance. However, some papers define DP as $\delta$DP, where $\delta$DP indicates the difference in positive predictions between sensitive groups. In this case, the smaller the difference, the better. 

**[Q6]** In Table 2, $S$ indicates the number of source domains, and $M$ signifies multiple domains. $D_{tgt}$ denotes target datasets. In order to highlight the novelty of this paper, Table 2 provides an overview of different settings of existing related approaches.
